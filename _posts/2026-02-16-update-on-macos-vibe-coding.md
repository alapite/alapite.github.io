---
author: Abiola Lapite
title: macOS Vibe Coding Update 
mathjax: false
tags:
- Large Language Models
- Machine Learning
- macOS
- Programming
- Swift
- GitHub
categories: programming
---
About a month ago, I [wrote about](https://alapite.github.io/programming/2026/01/20/vibe-coding-on-the-cheap.html) my attempts to replicate the authentic "vibe coding" experience without agreeing to pay Anthropic $200 every month for the pleasure. For the purpose of the exercise, I thought it best to settle on a combination of language and platform for which I had little experience, so that I would be in a similar boat to the "vibe coders" without any prior software development experience (or at least, as similar as one could hope for with any developer who has worked with a broad range of languages and frameworks over many years ...) Towards this end, I decided to try to prompt my way to a native macOS image browser written in Swift. 

In the course of this experiment, I got to try out a few of the leading Chinese models at the time (notably GLM 4.7 and Minimax M2.1), as well as Google's Gemini 3 Flash, which I found to offer no tangible benefits to offset its costing 10 times the Chinese alternatives on OpenRouter. The good news is that the experiment did succeed in meeting the goal of getting something built which met the functional requirements laid out at the beginning. The bad news is that **how** that code was written was clearly far from ideal, even to someone like me who hasn't touched Swift in the last 7 years.    

One immediately obvious problem was that the code generated by all three models contained absolutely no tests whatsoever. Even if one were to grant that testing practices differ by language and framework, it's hard to imagine that as according with good practice on any platform in broad use today.

Another problem which quickly became clear was that while the application did work, it did so in a decidedly laggy, sluggish manner, taking its time loading images even on a machine with lots of RAM to spare and extremely fast SSD storage. Clearly there was lots of scope here for performance improvements.

I also wondered whether the code was managing state in a manner that would be considered well-designed from a concurrency viewpoint. Perhaps a failure to do so was one reason why it seemed so slow? One could imagine, for example, a situation in which a thread meant only for UI updates was being regularly blocked to load images from disk storage.

The final issue which gnawed at me was whether the structure of the codebase was really up to scratch,
at least as far as Swift development standards go. Was it really acceptable to lay out all of the '*.swift' files at the root of the directory tree? Where were image assets like icon files supposed to go? 

As I'd heard good things about OpenAI's generous usage limits even on the $20/month "Plus" plan, I decided to sign up, and for my first usage, I decided to let Codex examine the current state of the project, look out for any issues, and make suggestions for improvements. Sure enough, Codex managed to spot all of the issues I'd mentioned, ranked them in order of decreasing priority, and then drew up a detailed plan to rectify all of them.

Having seen the questionable code that can come of having these models "one shot" a large amount of work in one go, I had Codex address the issues one at a time, halting after each one so I could carry out my own verification, and commit the latest updates to Git. This was the extent of my manual intervention, and I otherwise left Codex to implement its fixes as originally suggested.

The end result of this process was that I now had a project with substantial test coverage (and with tests that seemed meaningful), and a built application that was much more responsive (thanks to  judicious use of caching and background loading). The layout of the codebase also made better sense, and the Bash build script used for building the application had now been enhanced to accommodate code-signing. Finally, the README.md file accompanying the source code now provided detailed information on the project's architecture, layout and build requirements. The code is [freely available here](https://github.com/alapite/macos-image-browser).

What's the takeaway from all of this? I'd say it's that just knowing to ask these tools to abide by minimal coding standards can make a huge difference. Even if one doesn't know enough to determine **what** said standards might be, one can at the very least request that the agents take a critical look at the source code, and to flag any possible problems they might notice. Some judgement is still required to know when the results of this process can be safely ignored, but the odds are that taking such reporting seriously will leave the codebase in a much better state than it would have been under a pure "one shot and forget it" approach, no matter which choice of agent and harness one settles on.


