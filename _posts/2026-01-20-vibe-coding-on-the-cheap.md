---
author: Abiola Lapite
mathjax: true
tags:
  - Large Language Models
  - Machine Learning
  - Programming
categories: programming
---
Lately, there's been a lot of online buzz about the capabilities of Anthropic's ["Claude Code"](https://claude.com/product/claude-code) offering, and how it's supposedly making a tremendous difference to how developers are approaching their jobs. Going by what one sees on sites like Reddit and Hacker News, nothing less than a top-tier _Claude Code Max_ account will do to experience this wonderful new world, but we're hardly talking chicken change at £200/month. It may be that many of those raving about Claude Code are having their accounts paid for by their employers, but what if that isn't an option? Is there really no choice but to swallow any reservations and hand over all that money to Anthropic, no matter one's personal circumstances? 

What confuses things further is that there is a distinction to be made between _Claude Code_ as an *interface*, and the "Opus 4.5" model which is currently Anthropic's top tier offering. How much of the rhapsodising over the "Claude Code" experience is due to the model itself, and how much owes to the CLI wrapper which drives it? As Anthropic does not allow usage of the Claude Code CLI without a paid Claude Code subscription, it isn't really possible to investigate this question without spending at least £20/month for a highly restrictive "Pro" plan which is unusable for any truly professional purpose.

Fortunately, it turns out that alternatives to Claude Code do exist, and many of them are open-source. In particular, I've been examining [Zed](https://zed.dev/), [Cline](https://cline.bot/), [Kilo Code](https://kilo.ai/) and [OpenCode](https://opencode.ai/). All four tools offer the ability to integrate with a wide variety of models either directly to the companies offering them, or via third-parties such as [OpenRouter](https://openrouter.ai/). While all of these tools have the same subscription model as Anthropic/Claude and OpenAI/Codex, none of them compel users to subscribe to anything to use them. 

For my purposes, I've stuck mostly to using OpenRouter with a pre-paid balance, which allows me to switch between model providers at will even during a single coding session. To keep costs manageable, I've stayed clear of Anthropic's models, and only occasionally made use of [GPT 5.2](https://openai.com/index/introducing-gpt-5-2/). Most of my initial "vibe coding" was done with [GLM 4.5 Air](https://huggingface.co/zai-org/GLM-4.5-Air), and then later with [GLM 4.7](https://huggingface.co/zai-org/GLM-4.7), but of late, my mainstay has been [Minimax M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1), which I've found to be superior to the GLM models in compliance with instructions, as well as in the quality of the code it generates, at least when the language is one of Swift, Python or Java. 

When it comes to how it feels to use these tools in practice, I can't say there's a great amount of difference, as they all seem to have converged on a workflow which I imagine was pioneered by Claude Code: there's a read-only "Plan" mode in which one can discuss various aspects of the model with the codebase, in the confidence that no changes will result, and then there's a "Build" mode in which one instructs the model to actually make changes, whether these be refactoring, augmenting existing code, creating new classes or something else. An important aspect of this "Plan" vs. "Build" distinction is that the model used in the "Plan" phase need **not** be the same as the one used in the "Build" phase. For example, one might use an expensive model like GPT 5.2 for planning, and then, once the plans are finalised, switch to a much cheaper model like Minimax M2.1 to execute on those plans.

In my initial attempts at using these tools to write code, I tried to play it safe by only making additive changes to codebases I already understood thoroughly (and which were in languages I knew well). None of the tools provide much guidance for how to make best use of large-language models as part of a structured development process, so I began by simply asking for what I wanted in as straightforward terms as possible, e.g. _"Use the Spring AI library to add an MCP server which can execute Google searches"_. 

Starting with the GLM 4.5 Air and 4.7 models, the initial results seemed impressive (especially given the voluminous log output of the "thinking" process), but on closer examination there were always issues, such as Spring annotations being applied at the wrong level (e.g. to a class rather than to a method in it), Lombok annotations being replaced with hardcoded uses of the SLF4J library, or project dependencies being rolled back to outdated versions, many of which had already been flagged as having security issues. 

Switching to Minimax M2.1 lessened the frequency of occurrence of such problems, but it didn't eliminate them completely, and I found myself spending so much time reviewing the code changes for insidious problems that it would have taken less time to have simply written the code by hand in the first place. Switching from Minimax 2.1 to Google's [Gemini 3 Flash](https://blog.google/products-and-platforms/products/gemini/gemini-3-flash/) didn't bring any appreciable benefit despite costing considerably more, and Google's model was being praised to the skies online not so long ago. Was this really what everyone was getting excited about? Was it a model issue, with Anthropic's Opus 4.5 that much better than the competition? Or did the problem lie in how I was working with these models?

Undaunted by these disappointments, I thought I'd try my hand at something more adventureous, by going full "YOLO" mode and vibe-coding something entirely from scratch, using a language and framework I was far from expert in, just as all the vibe-coding CEO types are said to be doing. Towards this end, I decided on creating a native MacOS image browser application in Swift. While I've had an Apple Developer licence for more than a decade, and did put a little effort into learning Swift many years ago, I haven't had any real reason to write any Swift in all of that time, nor seen any need to create Mac-native applications. I would have to rely completely on these tools to autogenerate something that worked, without the option of jumping in and fixing any blocking issues that might arise.

As before, I had no template or workflow scaffolding to work with, and relied entirely on simply asking for what I wanted through the OpenCode interface (which I settled on to simulate the Claude Code experience as closely as possible). Having created an empty project directory, from within it, I asked GLM 4.7 to write a Swift-based, native MacOS image browser application, which also had to support
1. autoloading of entire directories,
2. manual and time-based slideshows, 
3. ordering of images by date, by name, or by arbitrary user choice.

GLM 4.7 promptly set about creating the code, printing out lots of relevant-seeming output as part of its "thinking" trace, and at the end I had a bunch of Swift files, as well as a compiled "ImageBrowser.app" to run. The final report from the model suggested it had not just successfully compiled the app, but had also successfully run it, but when I tried to do the same, I kept seeing the same error: apparently the generated binary hadn't been [code-signed](https://support.apple.com/en-gb/guide/security/sec3ad8e6e53/web), in violation of Apple's security guidelines. This should not have been an issue, seeing as
1. I have an Apple Developer licence, as mentioned earlier, and
2. I was compiling the application on my own machine, and solely for my own use.

Pointing out the issue did nothing to fix it while using GLM 4.7, but then I switched to MiniMax 2.1. Its analysis of the codebase indicated I could resolve the problem by migrating to a more modern  project format, which would then enable the creation of a build script incorporating code-signing. While it wasn't clear how much of this was correct, I **was** operating in YOLO mode, after all, so I simply approved the suggested changes and sat back, awaiting the results. And guess what? **It worked!**

Yes, I had now successfully created a fully functional, Mac-native image-browser from scratch, in less than an hour, simply by typing in a few prompts, and all of that had cost me less than $2.00 via OpenRouter. The process hadn't been completely trouble-free, but it was impressive nonetheless, as it would have taken me at _least_ a week or two to brush up enough on modern Swift and MacOS development to even _think_ of attempt doing something like this unaided, and even then there's no guarantee I would have had something working for many more days afterward. I say all of this as someone who's been writing code for more than 2 decades, so I can only imagine how magical the capabilities of these new tools must seem to people without years of programming experience or a computer-science education.

Having said all of the above, there were some obvious issues that immediately presented themselves. Not least of these issues was that I had no idea how well all of this autogenerated code followed modern best-practices when writing Swift apps for Macs. I did use Gemini 3 Flash to review the code and flag any likely issues, but how much credence could I really put in the suggestions it returned, given my own lack of experience in this particular domain? Could I just reiterate on the reviews until I got the all-clear?

What quickly became apparent, once I started trying to use these models to review code, is that they will often take the very fact that a review is requested as an indication that problems exist even where there aren't any. Matters aren't helped by the fact that there are many aspects of programming on which no consensus exists, so one can end up with an endless chain of LLM-generated reviews in which each successive review takes the opposite point-of-view to the one before it.

One issue that would be glaringly obvious to any developer, even one who had never before seen a single line of Swift, is that the code generated for the image-browser had absolutely **no accompanying tests**. While I'm unfamiliar with what currently passes for acceptable testing in the iOS/MacOS world, I do know that the right number of tests shouldn't be **zero**. While I could certainly have done a bit more prompting to get one model or another to generate some test coverage, I would still have been left with the question of determining just how meaningful the new tests were. How could I know that I wasn't being given tests which could never fail under any conditions, or which tested something subtlely different from what I ought to care about? Again, the only way I can see out of this conundrum is for some human with subject-area expertise to keep an eye out for such mistakes.   

To put things plainly, if this were a project meant for commercial use, or at least intended for a wider audience than myself, I would very much want someone with experience in the loop to provide the sanity checking that I couldn't, as I am sure there must be any number of serious issues in the code of which I am utterly unaware, just as even the small changes I requested in my initial efforts with Java and Python had their own problems that only a vigilant and expert eye would have noticed. Unstructured "vibe-coding" might suffice for proofs-of-concept and disposable tools for strictly personal use, but I wouldn't trust it outside of those use cases. But then again, maybe Claude Code and Opus 4.5 really is just that much better than anything else, and my doubts only reflect my attempts at replicating the experience on the cheap ... 

PS: I have now [uploaded the code](https://github.com/alapite/macos-image-browser) for the image-browser application, just in case anyone is curious about what it looks like. _"Caveat emptor"_, as the saying goes.