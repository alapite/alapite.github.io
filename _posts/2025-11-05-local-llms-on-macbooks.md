---
author: Abiola Lapite
tags: llms
categories: machine-learning
---
Over the last few months I've been giving a lot of thought to the question of how to go about running large language models (LLMs) fully locally, rather than writing wrapper code around calls to one of the major vendors. This article tries to gather together some of the insights I've gained in the process of discovery, in the hope of saving at least a few souls some of the fumbling around and trips down blind-alleys that I had to go through.

The first thing worth mentioning right at the outset: all of what follows is about using **pre-trained** models locally, particularly for **inference**. The cost of **training** cutting-edge LLMs from scratch now runs into the tens or hundreds of millions of $US Dollars, and only the most well-funded companies (almost all of which are American or Chinese) can afford to do so. It isn't simply a question of lavishing huge sums of money on hardware either, as huge amounts of effort must go into collecting all the data (including dealing with legal questions of provenance, augmenting it with synthetic data, etc.), creating an evaluation pipeline, and much else besides. At this point in time, taking an existing model off-the-shelf and tweaking it to fit one's purposes is really the only sensible option for most would-be users of LLMs.  

While we're on the topic of tweaking LLMs, I should clarify that I'm also not going to spend any time discussing local **fine-tuning** in this post. While fine-tuning pre-existing models can be much less expensive than creating them from scratch (particularly if one utilises techniques like [QLoRA](https://arxiv.org/abs/2305.14314)), that still isn't something most people should consider, locally or otherwise. Acquiring enough high-quality data to properly fine-tune is neither cheap nor easy, and even then one needs to have in place an evaluation process comprehensive enough to ensure that the fine-tuning achieves its intended goals without also worsening the model's performance in other important respects ([catastrophic forgetting](https://www.ibm.com/think/topics/catastrophic-forgetting)). 

Meanwhile, there's the significant risk that all that effort will turn out to be wasted, should the fine-tuned model be quickly rendered obsolete by new releases from the major American or Chinese model vendors. Industry consensus right now seems to be that fine-tuning is almost never justifiable in practice, unless it's being done for very small base models intended for use in very restricted, resource-constrained contexts (e.g. object classification in a smartphone app).

## Why Run Anything Locally?

### What's the Goal?
If the goal is simply to have access to an LLM to act as a personal oracle, then there's really no need to look beyond what's already available through sites like [ChatGPT](https://chatgpt.com), [Google Gemini](https://gemini.google.com/) or [Claude](https://claude.ai). The offerings from major vendors like Google, OpenAI and Anthropic are hosted on much more powerful hardware than most people could even dream of, they are hooked to vastly larger pools of data (which they go to great lengths to keep up to date), and they are supported by armies of specialists paid to keep them running trouble-free.

Even if your needs go beyond having access to an LLM you can use as a personal oracle, in most cases it's doubtful that you'll need to bother with locally hosting an LLM to achieve your goal, whatever it is. All of the major vendors provide developer APIs which can be called in a variety of languages, typically including - at a minimum - Python, TypeScript and Java (including [OpenAI for ChatGPT](https://platform.openai.com/), [Google for Gemini](https://ai.google.dev/), and and Anthropic for [Claude](https://docs.claude.com/en/home)). In addition, a huge ecosystem of third-party tools has emerged, leveraging these APIs to provide all kinds of extra functionality, from [Retrieval Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) (RAG) to multi-agent [orchestration](https://www.langchain.com/langgraph) and more. For anyone without peculiarly strict requirements (e.g. needing to abide by strict privacy laws), simply paying the subscription fees demanded by OpenAPI or one of its competitors will provide the maximum return on investment - there's no need to worry about infrastructure issues, it's possible to get started straightaway, and all the popular tooling can be expected to just work, right out of the box. 

Depending on what you're doing, paying for a subscription from a major will almost certainly make more sense than simply buying the hardware needed to run anything comparable locally, let alone the costs of hosting that hardware somewhere, paying for all the energy it will use, and paying for someone to make sure it stays running. A ChatGPT Pro subscription can [currently be had](https://chatgpt.com/pricing/) for $200/month with (supposedly) relatively relaxed limits on usage rates (as well as a 4x larger context-window). If you prefer a per-API/per-token pricing approach, sites like [yourgpt.ai](https://yourgpt.ai/tools/openai-and-other-llm-api-pricing-calculator) and [Price Per Token](https://pricepertoken.com/) indicating very reasonable prices per input/output token.

 Even if you aren't content with simply calling models served up by Anthropic, OpenAI and company, there's always the option of deploying open-sourced models on AWS, Google Cloud or Microsoft Azure. Beyond the major cloud infrastructure providers, there are specialist GPU-rental companies like [Runpod](https://www.runpod.io/) through which one can rent virtually any model of GPU on an ad-hoc basis, at much lower rates than one can get on AWS and its peers (for example, Runpod currently lists an Nvidia H200 with 141GB of HBM memory for just $3.59/hour).

Using a cloud-based provider starts to seem especially appealing if one is willing to go beyond the usual American companies, and look at the cutting-edge Chinese models from [Qwen](https://qwen.ai/apiplatform), [Z.ai](https://docs.z.ai/guides/overview/pricing) and [Deepseek](https://api-docs.deepseek.com/). The offerings from these companies are competitive in benchmarks with the best proprietary models from the American vendors, and word-of-mouth about their real world performance is also very favourable on forums like Reddit. Best of all, the Chinese companies typically make even their best models openly available, which means they can easily be through any of the GPU providers I mentioned above. A single rented H200 should be able to accommodate a fairly capable model like [GPT OSS 120b](https://huggingface.co/unsloth/gpt-oss-120b-GGUF) (albeit at 8-bit quantisation), or a 6-bit quantisation of [GLM-4.5-Air](https://huggingface.co/zai-org/GLM-4.5-Air). The more modest your model needs are, the lower the cost of renting the hardware to deploy it as needed.    

## When Local LLMs Make Sense
Having put so much effort into explaining why trying to run LLMs locally makes little sense for most people (and even for most developers), I'll now lay out the case for why one might wish to do so anyway.

### When Privacy is Absolutely Critical
One of the issues of going with the major API vendors is the question of what privacy guarantees they offer, and how much faith one can put in these promises. While I expect their biggest customers, being the types to have in-house lawyers, can negotiate water-tight guarantees to cover all scenarios, the average individual or small company spending a few hundred Dollars a month. The model vendors have very strong incentives to want to look at customer data for training purposes, given that they've essentially run out of freely available sources of data on the web. Carefully generated synthetic data can make up for this to some extent, but nothing beats real-world data provided by customers who put enough value in said data to be willing to pay someone else to process it. It's not hard to imagine that these vendors actually put a **higher** value on access to such data than they do on the subscription fees paid by individuals and small businesses. It wouldn't be a stretch for them to transition fully to Facebook-style business models, in which the true customers are advertisers, market researchers and the like, with the true end-product being the millions of end-users freely sharing their precious personal data in exchange for access to the LLM oracles.

Concerns about privacy become especially acute when the data in question is of a particularly sensitive nature, with correspondingly strict legal obligations on how said data can be handled (e.g. patients' medical records). In such cases, simple considerations of cost go out of the window, as does the option of hosting an open-weight model of one's choice on hardware rented from someone else. Here the only way forward is with a truly in-house solution, and the only question is whether what's being built will be worth all of the time, money and labour which will go into first setting up the in-house model platform, and subsequently maintaining it. 

Here "maintenance" means not just keeping all the hardware running (which also means constant monitoring to know when it **isn't** running as it should), but also keeping all data sources up to date, a framework suitable for running periodic, rigorous evaluations of the system, and the necessary observability to ensure that end-users continue to be satisfied with the results they get even as the platform has to be tweaked and updated over time. As all of the preceding suggests, the upfront hardware costs might actually turn out to be a minor fraction of the total expenditure required over the longer term.      

### When You Already Own the Hardware
The first consideration in favour of doing so would be already owning the necessary hardware anyway. If you're trying to run a frontier model - even an open-sourced one like GLM 4.6 - the odds of you meeting this requirement are pretty low, as [the full model](https://huggingface.co/zai-org/GLM-4.6) has 357 billion parameters, while even a heavily quantized, [2-bit version](https://docs.unsloth.ai/models/glm-4.6-how-to-run-locally) still requires both a GPU with 24GB of RAM **and** 128GB of main memory (growing to 205GB of RAM for the 4-bit version). 

Not everything requires the latest, top-of-the-line model, and if you have more modest goals in mind, it's quite possible that you meet the hardware threshold to try out a smaller model. For example, if all you're trying to do is carry out some sentiment analysis, then the [Llama 3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B) model might be all you need, and that should be easily runnable on any [M-series](https://en.wikipedia.org/wiki/Apple_silicon#M_series) Mac or MacBook with 16GB of RAM. 

I emphasise Macs here because of their unified memory architecture (UMA), which gives the machines' GPUs high-bandwidth access to all of the system's memory, rather than just a sliver of dedicated GPU RAM as has traditionally been the case with Intel-based machines. Couple this with the relatively high [memory-bandwidth available](https://www.apple.com/macbook-pro/specs/) on even the entry-level MacBook Pro models - starting at 153 GB/s for the new M5, and going up to 410 GB/s on the M4 Max versions - and what you have are machines capable of running inference at surprisingly low cost per Dollar (and per Watt). 

By way of contrast, even the newest Intel consumer machines with Dual Channel DDR5 RAM top out at 102 GB/s (with high-end, "workstation" machines hitting 204 GB/s). As the GPUs in Intel machines have typically their own separate, faster [GDDR5](https://en.wikipedia.org/wiki/GDDR5_SDRAM) memory, carrying out inference on such machines leads to a lot of extra traffic over that already constricted memory bus, as data must be constantly shuttled between the GPU RAM and system memory (whereas on UMA machines, the GPU and CPU can read-from/write-to exactly the same RAM). 

One consequence of this is that model developers have tried to keep as much work on GPU RAM as possible, which puts a hard limit on the size of models runnable on such architectures, regardless of how much system RAM they might possess. An NVIDIA GeForce GTX 1080 Ti card might have 484.4 GB/s, but it only has 11 GB of that high-bandwidth memory. This is even more constraining than it might seem at first glance, as not all of that memory can be used simply for loading model weights. Some memory must be set aside for handling query contexts, which can quickly balloon in size as they go through query rewriting, RAG and other steps commonly taken to improve the quality of results. 

#### Does Your Hardware have the Software Support?
There's no point spending lots of money on physical hardware if the software support isn't there in terms of libraries. NVIDIA is able to charge a huge premium for its GPUs because it has successfully spent the last decade building its proprietary [CUDA framework](https://en.wikipedia.org/wiki/CUDA) into a de-facto standard for doing what used to be called "deep-learning" (and is now sold as "AI"). AMD was very slow to put substantial effort into its [ROCm](https://en.wikipedia.org/wiki/ROCm) alternative, and the company offers no support for ROCm on its consumer-oriented offerings, in contrast to which NVIDIA (which already completely dominates on the high end). ROCm is therefore stuck in a "worst of both worlds" situation at present.

On the other hand, the situation with Apple Silicon is far more encouraging, thanks to its [MLX framework](https://mlx-framework.org/). This has seen lots of developer take-up, with a huge [HuggingFace community](https://huggingface.co/mlx-community) which usually provides MLX versions of most open-source models within days of release. For instance, one can already find [10 different MLX-specific quantisations](https://huggingface.co/models?other=base_model:quantized:zai-org%2FGLM-4.6&sort=trending&search=mlx) of the [GLM 4.6 model](https://huggingface.co/zai-org/GLM-4.6) which was only [announced](https://z.ai/blog/glm-4.6) on the 30th of September.  