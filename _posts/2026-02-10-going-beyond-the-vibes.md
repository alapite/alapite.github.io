---
author: Abiola Lapite
title: Going Beyond Vibe Coding
mathjax: true
tags:
  - Large Language Models
  - Machine Learning
  - Programming
categories: programming
---

In a [previous post](https://alapite.github.io/programming/2026/01/20/vibe-coding-on-the-cheap.html), I described my experiences vibe-coding a native macOS image-browser app. As I mentioned, it took remarkably little effort to create the app, as all I really had to do was provide a list of requirements, leaving the agent to get to work implementing them. Granted, I did initially run into an issue with how GLM 4.7 set up the project, but that seems to me to have been a model-specific issue, perhaps reflecting a deficit in its training data. Minimax 2.1 did a quick job of fixing the project setup issue, and I'm pretty confident that Claude Opus or any of the GPT 5.2 models would have done the whole job right from the outset. 

In short, based on my own experiences and on examples I've seen elsewhere (e.g. in Alejandro AO's [GPT 5.3 Codex vs Opus 4.6 comparion video](https://www.youtube.com/watch?v=c31Ow23mErE)), it has never been easier to create fully functional applications and websites in just a few keystrokes, no programming experience or education required. Anyone willing to pay for a subscription (or to buy credits from a service like [OpenRouter](https://openrouter.ai)) can now accomplish in **hours** what would formerly have required paying one or more highly-paid professionals for at least a few weeks of their time. 

As appealing as this prospect might seem, however, it misses the forest for the trees, as it's one thing to autogenerate something that seems to works under minimal testing, and another thing entirely to do so in a manner which is easily maintainable once bug-reports start coming in, which is easily extendable as requests for additional features begin to arrive, which remains robust under conditions very different from what the  creator ever have envisioned, which continues to be secure under attacks so fiendish in their creativity that the average person would struggle to understand how anyone could have come up with said strategems. None of these strengths are the sorts of things foremost in mind when feeling awe at, say, Claude Opus managing to "one-shot" in a few hours whatever snazzy new thing one wanted. 

If the goal is to create throw-away software meant for very limited distribution, then it's possible that all of the concerns I've mentioned might be irrelevant, and that it's enough to simply make use of whatever Opus or Codex spit out (assuming said product at least passes basic manual testing). But something tells me that this "use it and lose it" scenario isn't actually that common in practice, and that even when it does match the original requirements, things almost always start to change if said "throw-away" tools begin to prove useful. I imagine that the way such things usually go will be that someone says _"This app/website is awesome, but I'd like just this really minor change or extra feature"_, and as such requests multiply, what was meant to be ephemeral begins to be regarded as an operational linchpin in need of long-term maintenance and extension. 

It's at this point in the product lifecycle that the lack of initial concern for old-school software engineering principles starts to take its toll, as each new request for a coding agent to make changes turns into an exercise in dread at the prospect that the agent will break something else while fulfilling the request. Regressions that would have been caught by unit-tests go overlooked, security issues that might have been flagged by static analysers are missed, concurrency issues that are thoroughly covered in [undergraduate textbooks](https://shop.elsevier.com/books/the-art-of-multiprocessor-programming/herlihy/978-0-12-415950-1) begin to pop up in the usual maddening way such problems do (which is to say "just intermittently enough that they are maddening to reproduce"), rookie-level [data-leaks](https://www.reddit.com/r/programming/comments/1meyjjx/tea_app_hack_disassembling_the_ridiculous_app/) give rise to widespread public embarrassment (or [worse](https://www.wiz.io/blog/exposed-moltbook-database-reveals-millions-of-api-keys)) ...  

In saying all of the preceding, I am not trying to point the finger of blame at the coding agents being provided by Anthropic, OpenAI, Z.ai, Minimax and many other vendors. It's not as if these coding agents are incapable of creating maintainable and extensible code, far from it. The best available agents can do an excellent job of creating well-designed and properly-tested code when asked to do so, but therein lies the rub: **they actually have to be explicitly asked to create such code**. How many of the executive and marketing types most excited about vibe-coding would even know to do so, let alone what exactly to ask for?

## The Bare Minimum Required

I'm going to outline a few basic steps that will save a lot of pain down the road, even if they'll be old hat to most software developers with even a modicum of commercial experience. 

### Basic Manual Testing
It ought to go without saying that the very least anyone can do, when using coding agents to create applications, is to at least do some basic manual verification that it works at all. This might seem redundant to mention, but the fact is that these agents are perfectly capable of providing output suggesting that they've put out something that works, only for one to find that said output won't even start! This was certainly my experience with GLM 4.7, which failed to even sign the app it compiled, yet provided output indicating it had actually verified the app to work by running it. Even after fixing the issue with Minimax 2.1, I still found that the resulting application was slow to load images, and that the keyboard shortcuts for the controls failed to work properly, with both issues requiring additional rounds of prompting (and, with the caching issue, a crash course in macOS [cache management](https://developer.apple.com/documentation/Foundation/NSCache), so I could understand the fix being proposed). 

Granted, more cutting-edge models would likely have fared better in this particular scenario, but sites like Reddit and BlueSky are littered with complaints about even the highly-lauded Opus 4.5 providing [misleading](https://www.reddit.com/r/vibecoding/comments/1r1kha5/vibe_coding_is_a_monkeys_paw_wish_and_nobodys/), hallucinatory output about the actions it has taken. I don't think this problem is ever likely to go away completely, and basic manual testing will remain a necessity for the forseeable future.

### Unit Testing
As far as basic quality controls go in software development, [unit testing](https://en.wikipedia.org/wiki/Unit_testing) is about as basic as it gets. The software development process is largely about keeping complexity in check by creating modular units one can treat as black boxes, so that they can be combined to create units of higher levels of abstraction. Good unit tests are one means of verifying that this modularity assumption is a safe one to make.

While manually writing unit-tests remains an option, it isn't a hard requirement. It's quite straightforward to prompt a coding agent to write unit-tests for a given chunk of code, and by and large they seem to do a good job of it as long as one is careful to verify their output, by actually reading the tests and checking that they are meaningful. Expertise in a particular programming language isn't to grasp the structure of a unit-test in the language, or to understand the assertions the test is trying to make. If the tests seem difficult to understand, one can prompt the agent to refactor them to be smaller and clearer. If doing so means the agent needs to also refactor the code being tested for greater clarity, all the better. The main thing is to ensure that the tests aren't the vacuous sort that continue to pass even when the code being tested breaks.

Taking this a step further, one could try to take [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) [TDD] seriously, by telling the agent to create all the unit-tests for the code it intends to implement **before** actually doing any implementation. The agent can be required to take a [red-green](https://www.codecademy.com/article/tdd-red-green-refactor) approach, verifying that the tests actually fail before the code they are testing is implemented. As long as the test-code itself remains unchanged, seeing the tests begin to pass will give a reliable signal that the implemented code works as expected. 

### Version Control
As one step to prevent issues like agents "cheating" on the TDD process, adding [version control](https://en.wikipedia.org/wiki/Version_control) of some sort is essential. Putting version control in place makes it easier to track precisely which code changes are making, and when. If the changes made by the agents are undesired, a good version-control system makes it quick and easy to roll the changes back to some prior "known to be good" state. 

For any new project adopting version control, the default option is likely to be [Git](https://git-scm.com/), and for very good reason. Git is extremely powerful, and has achieved near-universal adoption, which is why it has excellent tooling support, both in IDEs and in coding agents like Claude Code, Codex, Cursor and the like. Anyone who can articulate well-thought-out reasons to adopt some alternative to Git is unlikely to be in need of the advice I'm offering here anyway. 

The official Git website has clear and easy-to-follow instructions for installing the tool on a personal machine, and once installed, initialising a Git repository inside a source code directory is as simple as running 
```shell
git init .
```
from the command-line. Git is a feature-rich and powerful tool, but only a few of said features are needed to get most of its benefits, and the Git website provides links to [excellent resources](https://git-scm.com/learn) for learning more.

Beyond basic committing of code to Git, a good next step is to find somewhere to safely mirror the committed code. Machine failures do happen, and it's unlikely that any but the most diligent of us are making daily backups of our machines which we then ship off to physically distinct locations. If the code is important to be worth committing, then it is important enough to back up to a site like [GitHub](https://github.com/), [GitLab](https://about.gitlab.com/) or [Codeberg](https://codeberg.org/). Deciding between these alternatives comes down to what terms and conditions one is willing to accept (for example, not everyone can live with GitHub's requirement that all repositories made by non-paying users must be public). Once said decision is made, however, the process for backing up committed code is pretty much the same for all of them. 

Another benefits of using Git is the facilities it provides for "tagging" particular code commits. A typical Git commit is a 128-bit hash string which says nothing informative to a user about what's being committed, and while the accompanying commit message can (and really, should) contain such information, it's often the case that one will only know that a specific commit represents a particularly stable point in the development process long after the commit has been made. Rather than requiring that users go back and re-edit older commits to attach such information, Git's tagging feature enables one to effectively say _"commit 4b75783g...1952bf6 passed all tests and can be used for release version X.Y.Z"_. Attaching a tag to a particular Git commit is as simple as, for example,
```shell
git tag -a '1.0.4'
```
where '1.0.4' here represents the application version under a [semantic versioning](https://semver.org/) approach.