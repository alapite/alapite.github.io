---
author: Abiola Lapite
title: Going Beyond Vibe Coding
mathjax: true
tags:
  - Large Language Models
  - Machine Learning
  - Programming
categories: programming
---

In a [previous post](https://alapite.github.io/programming/2026/01/20/vibe-coding-on-the-cheap.html), I described my experiences vibe-coding a native macOS image-browser app. As I mentioned, it took remarkably little effort to create the app, as all I really had to do was provide a list of requirements, leaving the agent to get to work implementing them. Granted, I did initially run into an issue with how GLM 4.7 set up the project, but that seems to me to have been a model-specific issue, perhaps reflecting a deficit in its training data. Minimax 2.1 did a quick job of fixing the project setup issue, and I'm pretty confident that Claude Opus or any of the GPT 5.2 models would have done the whole job right from the outset. 

In short, based on my own experiences and on examples I've seen elsewhere (e.g. in Alejandro AO's [GPT 5.3 Codex vs Opus 4.6 comparion video](https://www.youtube.com/watch?v=c31Ow23mErE)), it has never been easier to create fully functional applications and websites in just a few keystrokes, no programming experience or education required. Anyone willing to pay for a subscription (or to buy credits from a service like [OpenRouter](https://openrouter.ai)) can now accomplish in **hours** what would formerly have required paying one or more highly-paid professionals for at least a few weeks of their time. 

As appealing as this prospect might seem, however, it misses the forest for the trees, as it's one thing to autogenerate something that seems to works under minimal testing, and another thing entirely to do so in a manner which is easily maintainable once bug-reports start coming in, which is easily extendable as requests for additional features begin to arrive, which remains robust under conditions very different from what the  creator ever have envisioned, which continues to be secure under attacks so fiendish in their creativity that the average person would struggle to understand how anyone could have come up with said strategems. None of these strengths are the sorts of things foremost in mind when feeling awe at, say, Claude Opus managing to "one-shot" in a few hours whatever snazzy new thing one wanted. 

If the goal is to create throw-away software meant for very limited distribution, then it's possible that all of the concerns I've mentioned might be irrelevant, and that it's enough to simply make use of whatever Opus or Codex spit out (assuming said product at least passes basic manual testing). But something tells me that this "use it and lose it" scenario isn't actually that common in practice, and that even when it does match the original requirements, things almost always start to change if said "throw-away" tools begin to prove useful. I imagine that the way such things usually go will be that someone says _"This app/website is awesome, but I'd like just this really minor change or extra feature"_, and as such requests multiply, what was meant to be ephemeral begins to be regarded as an operational linchpin in need of long-term maintenance and extension. 

It's at this point in the product lifecycle that the lack of initial concern for old-school software engineering principles starts to take its toll, as each new request for a coding agent to make changes turns into an exercise in dread at the prospect that the agent will break something else while fulfilling the request. Regressions that would have been caught by unit-tests go overlooked, security issues that might have been flagged by static analysers are missed, concurrency issues that are thoroughly covered in [undergraduate textbooks](https://shop.elsevier.com/books/the-art-of-multiprocessor-programming/herlihy/978-0-12-415950-1) begin to pop up in the usual maddening way such problems do (which is to say "just intermittently enough that they are maddening to reproduce"), rookie-level [data-leaks](https://www.reddit.com/r/programming/comments/1meyjjx/tea_app_hack_disassembling_the_ridiculous_app/) give rise to widespread public embarrassment (or [worse](https://www.wiz.io/blog/exposed-moltbook-database-reveals-millions-of-api-keys)) ...  

In saying all of the preceding, I am not trying to point the finger of blame at the coding agents being provided by Anthropic, OpenAI, Z.ai, Minimax and many other vendors. It's not as if these coding agents are incapable of creating maintainable and extensible code, far from it. The best available agents can do an excellent job of creating well-designed and properly-tested code when asked to do so, but therein lies the rub: **they actually have to be explicitly asked to create such code**. How many of the executive and marketing types most excited about vibe-coding would even know to do so, let alone what exactly to ask for?

## The Bare Minimum Required

### Basic Manual Testing
It ought to go without saying that the very least anyone can do, when using coding agents to create applications, is to at least do some basic manual verification that it works at all. This might seem redundant to mention, but the fact is that these agents are perfectly capable of providing output suggesting that they've put out something that works, only for one to find that said output won't even start! This was certainly my experience with GLM 4.7, which failed to even sign the app it compiled, yet provided output indicating it had actually verified the app to work by running it. Even after fixing the issue with Minimax 2.1, I still found that the resulting application was slow to load images, and that the keyboard shortcuts for the controls failed to work properly, with both issues requiring additional rounds of prompting (and, with the caching issue, a crash course in macOS [cache management](https://developer.apple.com/documentation/Foundation/NSCache), so I could understand the fix being proposed). 

Granted, more cutting-edge models would likely have fared better in this particular scenario, but sites like Reddit and BlueSky are littered with complaints about even the highly-lauded Opus 4.5 providing misleading hallucinatory output about the actions it has taken. I don't think this problem is ever likely to go away completely, and basic manual testing will remain a necessity for the forseeable future.

### Unit Testing
As far as basic quality controls go in software development, [unit testing](https://en.wikipedia.org/wiki/Unit_testing) is about as basic as it gets. The software development process is largely about keeping complexity in check by creating modular units one can treat as black boxes, so that they can be combined to create units of higher levels of abstraction. Good unit tests are one means of verifying that this modularity assumption is a safe one to make.

While manually writing unit-tests remains an option, it isn't a hard requirement. It's quite straightforward to prompt a coding agent to write unit-tests for a given chunk of code, and by and large they seem to do a good job of it as long as one is careful to verify their output, by actually reading the tests and checking that they are meaningful. Expertise in a particular programming language isn't to grasp the structure of a unit-test in the language, or to understand the assertions the test is trying to make. If the tests seem difficult to understand, one can prompt the agent to refactor them to be smaller and clearer. If doing so means the agent needs to also refactor the code being tested for greater clarity, all the better. The main thing is to ensure that the tests aren't the vacuous sort that continue to pass even when the code being tested breaks.

Taking this a step further, one could try to take [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) seriously, by telling the agent to create all the unit-tests for the code it intends to implement **before** actually doing any implementation. The agent can be required to take a [red-green](https://www.codecademy.com/article/tdd-red-green-refactor) approach, verifying that the tests actually fail before the code they are testing is implemented. As long as the test-code itself remains unchanged, seeing the tests begin to pass will give a reliable signal that the implemented code works as expected.  