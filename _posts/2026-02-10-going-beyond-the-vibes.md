---
author: Abiola Lapite
title: Going Beyond Vibe Coding
mathjax: true
tags:
  - Large Language Models
  - Machine Learning
  - Programming
categories: programming
---

In a [previous post](https://alapite.github.io/programming/2026/01/20/vibe-coding-on-the-cheap.html), I described my experiences vibe-coding a native macOS image-browser app. As I mentioned, it took remarkably little effort to create the app, as all I really had to do was provide a list of requirements, leaving the agent to get to work implementing them. Granted, I did initially run into an issue with how GLM 4.7 set up the project, but that seems to me to have been a model-specific issue, perhaps reflecting a deficit in its training data. Minimax 2.1 did a quick job of fixing the project setup issue, and I'm pretty confident that Claude Opus or any of the GPT 5.2 models would have done the whole job right from the outset. 

In short, based on my own experiences and on examples I've seen elsewhere (e.g. in Alejandro AO's [GPT 5.3 Codex vs Opus 4.6 comparion video](https://www.youtube.com/watch?v=c31Ow23mErE)), it has never been easier to create fully functional applications and websites in just a few keystrokes, no programming experience or education required. Anyone willing to pay for a subscription (or to buy credits from a service like [OpenRouter](https://openrouter.ai)) can now accomplish in **hours** what would formerly have required paying one or more highly-paid professionals for at least a few weeks of their time. 

As appealing as this prospect might seem, however, it misses the forest for the trees, as it's one thing to autogenerate something that seems to works under minimal testing, and another thing entirely to do so in a manner which is easily maintainable once bug-reports start coming in, which is easily extendable as requests for additional features begin to arrive, which remains robust under conditions very different from what the  creator ever have envisioned, which continues to be secure under attacks so fiendish in their creativity that the average person would struggle to understand how anyone could have come up with said strategems. None of these strengths are the sorts of things foremost in mind when feeling awe at, say, Claude Opus managing to "one-shot" in a few hours whatever snazzy new thing one wanted. 

If the goal is to create throw-away software meant for very limited distribution, then it's possible that all of the concerns I've mentioned might be irrelevant, and that it's enough to simply make use of whatever Opus or Codex spit out (assuming said product at least passes basic manual testing). But something tells me that this "use it and lose it" scenario isn't actually that common in practice, and that even when it does match the original requirements, things almost always start to change if said "throw-away" tools begin to prove useful. I imagine that the way such things usually go will be that someone says _"This app/website is awesome, but I'd like just this really minor change or extra feature"_, and as such requests multiply, what was meant to be ephemeral begins to be regarded as an operational linchpin in need of long-term maintenance and extension. 

It's at this point in the product lifecycle that the lack of initial concern for old-school software engineering principles starts to take its toll, as each new request for a coding agent to make changes turns into an exercise in dread at the prospect that the agent will break something else while fulfilling the request. Regressions that would have been caught by unit-tests go overlooked, security issues that might have been flagged by static analysers are missed, concurrency issues that are thoroughly covered in [undergraduate textbooks](https://shop.elsevier.com/books/the-art-of-multiprocessor-programming/herlihy/978-0-12-415950-1) begin to pop up in the usual maddening way such problems do (which is to say "just intermittently enough that they are maddening to reproduce"), rookie-level [data-leaks](https://www.reddit.com/r/programming/comments/1meyjjx/tea_app_hack_disassembling_the_ridiculous_app/) give rise to widespread public embarrassment (or [worse](https://www.wiz.io/blog/exposed-moltbook-database-reveals-millions-of-api-keys)) ...  

In saying all of the preceding, I am not trying to point the finger of blame at the coding agents being provided by Anthropic, OpenAI, Z.ai, Minimax and many other vendors. It's not as if these coding agents are incapable of creating maintainable and extensible code, far from it. The best available agents can do an excellent job of creating well-designed and properly-tested code when asked to do so, but therein lies the rub: **they actually have to be explicitly asked to create such code**. How many of the executive and marketing types most excited about vibe-coding would even know to do so, let alone what exactly to ask for?

## The Bare Minimum Required

I'm going to outline a few basic steps that will save a lot of pain down the road, even if they'll be old hat to most software developers with even a modicum of commercial experience. 

### Basic Manual Testing
It ought to go without saying that the very least anyone can do, when using coding agents to create applications, is to at least do some basic manual verification that it works at all. This might seem redundant to mention, but the fact is that these agents are perfectly capable of providing output suggesting that they've put out something that works, only for one to find that said output won't even start! This was certainly my experience with GLM 4.7, which failed to even sign the app it compiled, yet provided output indicating it had actually verified the app to work by running it. Even after fixing the issue with Minimax 2.1, I still found that the resulting application was slow to load images, and that the keyboard shortcuts for the controls failed to work properly, with both issues requiring additional rounds of prompting (and, with the caching issue, a crash course in macOS [cache management](https://developer.apple.com/documentation/Foundation/NSCache), so I could understand the fix being proposed). 

Granted, more cutting-edge models would likely have fared better in this particular scenario, but sites like Reddit and BlueSky are littered with complaints about even the highly-lauded Opus 4.5 providing [misleading](https://www.reddit.com/r/vibecoding/comments/1r1kha5/vibe_coding_is_a_monkeys_paw_wish_and_nobodys/), hallucinatory output about the actions it has taken. I don't think this problem is ever likely to go away completely, and basic manual testing will remain a necessity for the forseeable future.

### Unit Testing
As far as basic quality controls go in software development, [unit testing](https://en.wikipedia.org/wiki/Unit_testing) is about as basic as it gets. The software development process is largely about keeping complexity in check by creating modular units one can treat as black boxes, so that they can be combined to create units of higher levels of abstraction. Good unit tests are one means of verifying that this modularity assumption is a safe one to make.

While manually writing unit-tests remains an option, it isn't a hard requirement. It's quite straightforward to prompt a coding agent to write unit-tests for a given chunk of code, and by and large they seem to do a good job of it as long as one is careful to verify their output, by actually reading the tests and checking that they are meaningful. Expertise in a particular programming language isn't to grasp the structure of a unit-test in the language, or to understand the assertions the test is trying to make. If the tests seem difficult to understand, one can prompt the agent to refactor them to be smaller and clearer. If doing so means the agent needs to also refactor the code being tested for greater clarity, all the better. The main thing is to ensure that the tests aren't the vacuous sort that continue to pass even when the code being tested breaks.

Taking this a step further, one could try to take [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) [TDD] seriously, by telling the agent to create all the unit-tests for the code it intends to implement **before** actually doing any implementation. The agent can be required to take a [red-green](https://www.codecademy.com/article/tdd-red-green-refactor) approach, verifying that the tests actually fail before the code they are testing is implemented. As long as the test-code itself remains unchanged, seeing the tests begin to pass will give a reliable signal that the implemented code works as expected. 

When scrutinising the unit-tests agents produce, it isn't enough to simply check that they are asserting that units of code work as expected when given the inputs they expect (in other words, **"happy path"** scenarios). It's also vital to check that the tests cover how the code being tested behaves when given unexpected or badly formed inputs, when some of the inputs don't actually exist, etc. Does the code properly throw or handle exceptions? Does it bother to log errors? Does it properly reject missing or incorrect parameters (e.g. security credentials)? A large part of the professional software development lifecycle involves probing such edge-cases, and verifying that code remains robust when things don't go as planned. One particularly helpful but woefully under-utilised[^1] technique for this is called [property-based testing](https://dev.to/keploy/property-based-testing-a-comprehensive-guide-lc2).    

### Version Control
As one step to prevent issues like agents "cheating" on the TDD process, adding [version control](https://en.wikipedia.org/wiki/Version_control) of some sort is essential. Putting version control in place makes it easier to track precisely which code changes are making, and when. If the changes made by the agents are undesired, a good version-control system makes it quick and easy to roll the changes back to some prior "known to be good" state. 

For any new project adopting version control, the default option is likely to be [Git](https://git-scm.com/), and for very good reason. Git is extremely powerful, and has achieved near-universal adoption, which is why it has excellent tooling support, both in IDEs and in coding agents like Claude Code, Codex, Cursor and the like. Anyone who can articulate well-thought-out reasons to adopt some alternative to Git is unlikely to be in need of the advice I'm offering here anyway. 

The official Git website has clear and easy-to-follow instructions for installing the tool on a personal machine, and once installed, initialising a Git repository inside a source code directory is as simple as running 
```shell
git init .
```
from the command-line. Git is a feature-rich and powerful tool, but only a few of said features are needed to get most of its benefits, and the Git website provides links to [excellent resources](https://git-scm.com/learn) for learning more.

Beyond basic committing of code to Git, a good next step is to find somewhere to safely mirror the committed code. Machine failures do happen, and it's unlikely that any but the most diligent of us are making daily backups of our machines which we then ship off to physically distinct locations. If the code is important to be worth committing, then it is important enough to back up to a site like [GitHub](https://github.com/), [GitLab](https://about.gitlab.com/) or [Codeberg](https://codeberg.org/). Deciding between these alternatives comes down to what terms and conditions one is willing to accept (for example, not everyone can live with GitHub's requirement that all repositories made by non-paying users must be public). Once said decision is made, however, the process for backing up committed code is pretty much the same for all of them. 

Another benefit of using Git is how it enables the "tagging" of particular code commits. A typical Git commit is a 128-bit hash string which says nothing informative to a user about what's being committed, and while the accompanying commit message can (and really, should) contain such information, it's often the case that one will only know that a specific commit represents a particularly stable point in the development process long after the commit has been made. Rather than requiring that users go back and re-edit older commits to attach such information, Git's tagging feature enables one to effectively say _"commit 4b75783g...1952bf6 passed all tests and can be used for release version X.Y.Z"_. Attaching a tag to a particular Git commit is as simple as, for example,
```shell
git tag -a '1.0.4'
```
where '1.0.4' here represents the application version under a [semantic versioning](https://semver.org/) approach.

The initial steps in Git usage above will probably suffice for absolute beginners working on solo projects, and, if the repositories are regularly pushed to remote locations as I've suggested, should be enough to make even the worst [horror](https://www.reddit.com/r/accelerate/comments/1lzv46u/gemini_pro_rm_rf_d_my_computer/) [scenarios](https://www.reddit.com/r/VibeCodeDevs/comments/1mod16k/claude_code_just_deleted_my_entire_project_with/) easy to recover from. However, once projects start to grow beyond the scale of a single developer working on one isolated feature or bugfix at a time, another of Git's features comes into play, namely [branching](https://git-scm.com/book/en/v2/Git-Branching-Basic-Branching-and-Merging). This particular feature really set Git head and shoulders above earlier version control systems like [Subversion](https://en.wikipedia.org/wiki/Apache_Subversion) (which formerly reigned supreme, but now - mercifully - has virtually no new adoption). Compared to preceding version control tools, Git's [3-way merging](https://blog.git-init.com/the-magic-of-3-way-merge/) strategy turned what had been a huge headache[^2] into a relatively painless experience. 

Learning how to make basic use of Git's branch-management features isn't hard, as [this excellent tutorial](https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell) illustrates. However, getting the best out of these powerful features requires some thought, especially when working on codebases where multiple bugfixes and feature enhancements can be in development at the same time. Things get even messier if the application being worked on can have several official releases outstanding simultaneously, each of which is backed by promises of long-term support[^3]. This is where a careful choice of branching strategy becomes important, and while tools like Claude Code and OpenAI Codex should be able to comply with any such strategy they are told to adopt, these agents will not do so out-of-the-box without explicit prompting.

## What Comes Next?
As I've tried to make clear from the start, the suggestions I've made here only constitute the barebones of what I think is needed to avoid a "vibe-coded" application from quickly degenerating into a mess of unmaintainable [spaghetti code](https://en.wikipedia.org/wiki/Spaghetti_code). I could write literally dozens of follow-on articles about the many important aspects of software development which I've left out here, but I doubt the typical "vibe coding" enthusiast would have any interest in reading them, even if I had the stamina to do so. All I'll say for now is that one piece of relatively low-hanging fruit to consider is to adopt some kind of formalised **process** to give structure to how new development is done, and that this doesn't need to be anywhere as process and ceremony-heavy as the [Scrum](https://en.wikipedia.org/wiki/Scrum_(project_management)) type of arrangement widely practiced in professional software enterprises. 

One possible starting point is to look at [articles](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/) on [Spec Driven Development](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/) for [inspiration](https://blog.jetbrains.com/junie/2025/10/how-to-use-a-spec-driven-approach-for-coding-with-ai/). I won't bother listing any particular toolkits, as this is an area witnessing rapid change, but I think the general principles these tools all share are in themselves sound, namely,
1. Start with gathering clear **requirements**. Don't proceed any further until these are absolutely clear.
2. Once the requirements are clear, draw up an **implementation plan** laying out how the requirements can be achieved in an **incremental** fashion. 
3. Only **after** the first 2 points are settled should agents be tasked with carrying out the actual **implementation**.

That last step should **not** be conceived of as simply handing Claude Code (or an alternative) some giant string of text with all the info from the prior steps, and then leaving the agent to "one shot" the entire thing over a few hours while one attends to something else. Instead, a better way to see things is that step 3 kicks of an iterative process, in which the agent tackles a new increment of the plan, some verification is carried out that the implementation matches up to what was planned, the plan is updated based on the results of this verification, after which the agent starts on the next chunk of work. The size of the increments in which to work can easily be adjusted to suit most tastes, depending on just how much trust one wishes to put in the agent. 

[^1]: It's astonishing how little used property-based testing is even within dedicated software businesses.
[^2]: A headache developers mostly tried to avoid by all working on the same branch, even if doing so meant that each person's commit ended up breaking the codebase, bringing everyone else's work to a halt until the issue was resolved.
[^3]: Consider, for example, operating system vendors like Microsoft or Apple, who have to provide security patches for both their latest offerings and one or more older ones which are not yet in the "End of Life" phase. For example, Apple's security updates for macOS Tahoe (released last year) are also provided for Sequioa, which was released 2 years ago.  