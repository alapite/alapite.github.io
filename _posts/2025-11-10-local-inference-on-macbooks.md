---
author: Abiola Lapite
tags: llms
categories: machine-learning
---
In my [previous post](2025-11-10-local-inference-on-macbooks.md) I laid out some issues to keep in mind when deciding whether it is worth going to the trouble of running LLMs locally. As I tried to make clear, the answer in most cases is almost certainly "No". The answer is only clearly "Yes" when 
1. privacy considerations come into the picture, or 
2. one already owns the necessary hardware, which also has the needed software support.

At present, the largest pool of individuals for whom the second case applies will be those who own [M-Series Macs](https://en.wikipedia.org/wiki/Apple_silicon#M-series_SoCs) with sufficient amounts of RAM (i.e. 24 GB or more). It's certainly possible to try out running inference on machines with only 16 GB of RAM, but this will limit the possibilities to only very small models, as the same memory also has to accommodate the operating system and any other applications the user might be running.

### Machine Specs and Model Choice
The two primary factors determining what will be possible running LLMs on a Mac will be
1. How much RAM the machine has, and
2. How much memory bandwidth the system provides

Obviously, in both cases, the more, the better. Other factors do also matter, e.g. the number of GPU cores packaged on the machine's SoC, and their generation (with GPU performance jumping about 35% going from M4 to M5, for example). Still, these are second-order considerations, with the sheer amount of RAM and the speed of access to it being most important (though this well might change once M5-Pro and M5-Max machines are released next year). 

Given how Apple's pricing structure works, there's really no good way to maximise the amount of RAM on a machine in any given line without also maximising the amount of memory bandwidth available for that RAM. In a pre-LLM world this would have been regarded as a huge demerit for Apple's offerings, but for the purposes of the current discussion, this pricing strategy can probably now be considered a **good** thing. I say this because it means people looking at getting 64, 96, 128 or even 256 GB of RAM won't be misled into thinking they'll be getting maximum value for their money by opting for build configurations without the bandwidth to get the most out of all that extra memory. 

To provide a concrete example, at present the maximum amount of RAM one can get with a 16-inch M4-Pro Macbook [is 48GB](https://www.apple.com/uk/shop/buy-mac/macbook-pro/16-inch-space-black-standard-display-apple-m4-pro-with-14-core-cpu-and-20-core-gpu-48gb-memory-512gb). Going up to 64 GB or higher requires selecting [an M4-Max](https://www.apple.com/uk/shop/buy-mac/macbook-pro/16-inch-space-black-standard-display-apple-m4-max-with-16-core-cpu-and-40-core-gpu-48gb-memory-1tb), but this also bumps up the available memory bandwidth from **273GB/s** to **546GB/s**. Even this higher number pales in comparison the 1,790GB/s available on an NVIDIA RTX 5090 card, but the NVIDIA card alone is currently listed at between £2,300 to £3,000 (assuming it's even available), and it comes with only 32 GB of that ultra-fast memory. In addition, the NVIDIA card can draw up to **575W** of power at peak usage, while an entire MacBook Pro peaks at **140W**. The NVIDIA RTX 5090 appears to be out of stock everywhere.

A similar story can be told about Apple's Mac Studio machines. An M4-Max version of the Studio with [128 GB of RAM](https://www.apple.com/uk/shop/buy-mac/mac-studio/apple-m4-max-with-14-core-cpu-32-core-gpu-16-core-neural-engine-36gb-memory-512gb) currently lists for £3,600 on Apple's website. Going up to 256 GB requires also moving up to an [M3-Ultra](https://www.apple.com/uk/shop/buy-mac/mac-studio/apple-m3-ultra-with-28-core-cpu-60-core-gpu-32-core-neural-engine-96gb-memory-1tb) version of the Mac Studio, but this also pushes up the system bandwidth from 410GB/s to 819GB/s. An M3 Ultra Mac Studio with 256 GB of RAM costs just £5,800, while a system with 512 GB can be had for just £9,700. It might seem outrageous that I use the term "just" in juxtaposition with such prices, but they really do offer good value for money, as I shall now explain.

A model running on a unified-memory-architecture with less memory-bandwidth will typically run faster than it would on a system which has an ultra-high bandwidth graphics card which lacks sufficient memory to keep the whole memory on the GPU. The larger the share of the model that must be offloaded onto much slower system memory, the bigger the hit will be. This issue tends to be exacerbated on the Intel machines available to consumers because these are almost always dual-channel systems which top out at 50-70 GB/s. Even the new entry-level M5 MacBook pro offers 153 GB/s more than 2x this amount of bandwidth, and the huge gap in bandwidth since the M-series machines were introduced is one of the reasons why they have offered so much better performance than Intel systems since Apple launched the M1.

With the above in mind, it is important to note that no single NVIDIA card available on the market offers 256 GB of RAM, not even high-end cards like the H100, H200 and B200 which are the subject of export controls to China. The maximum amount of RAM provided on any single NVIDIA card is 192 GB, and for that much NVIDIA charges a princely sum. For example, various online sources mention a single [NVIDIA B200 SXM 192 GB](https://www.techpowerup.com/gpu-specs/b200-sxm-192-gb.c4210) as selling for **$45-50,000**. Granted, that huge sum buys memory of the _extremely_ fast HBM3 variety (providing an astonishing **8.2 TB/s** of bandwidth), but it still sets a much lower limit on how big a model one can load on a single card than Apple's Mac Studios do. 

Using multiple NVIDIA cards **does** make it possible to load larger models (which is how the trillion-parameter models which make the news are deployed), but so does connecting multiple Apple machines via Thunderbolt 5. In addition, Apple's machines are freely available to any members of the public willing to pay for them ( and with generous consumer financing for the suitably qualified), and the Apple products are typically delivered to customers within a month of being ordered. In contrast, NVIDIA's cards are routinely mentioned in news articles as sold out many months in advance of production, even with their astronomical price tags.

The long and short of it is that Apple's hardware is a lot more reasonably priced than it might first appear, at least for the purpose of running inference on LLMs locally. The virtues of Apple's offerings only increase when one thinks about the power draw of even consumer-level NVIDIA cards, the space needed for the PC boxes to house said cards, the power which will be drawn by the resulting systems, and all the accompanying heat and noise they will generate (which will in turn necessitate thinking about how to keep the systems cool). At peak utilisation, even the highest end M3 Ultra Mac Studio **systems** draw less than **half** the power needed for the consumer-oriented RTX 5090, and they can sustain this performance with only their (relatively-quiet) internal fans to keep temperatures down.

## Making the Best of What You Have

Of course, all of the preceding discussion is moot if the hardware decision has already been made (e.g. because one already own the machine in question). If the primary usage for your machine is as a home device, then it's doubtful it will have more than 48 GB of RAM, even if, like me, you're the sort to simultaneously run an IDE, a database and several Docker images while having dozens of browser tabs open.  