---
author: Abiola Lapite
tags: llms
categories: machine-learning
---
In my [previous post](2025-11-10-local-inference-on-macbooks.md) I laid out some issues to keep in mind when deciding whether it is worth going to the trouble of running LLMs locally. As I tried to make clear, the answer in most cases is almost certainly "No". The answer is only clearly "Yes" when 
1. privacy considerations come into the picture, or 
2. one already owns the necessary hardware, which also has the needed software support.

At present, the largest pool of individuals for whom the second case applies will be those who own [M-Series Macs](https://en.wikipedia.org/wiki/Apple_silicon#M-series_SoCs) with sufficient amounts of RAM (i.e. 24 GB or more). It's certainly possible to try out running inference on machines with only 16 GB of RAM, but this will limit the possibilities to only very small models, as the same memory also has to accommodate the operating system and any other applications the user might be running.

### Machine Specs and Model Choice
The two primary factors determining what will be possible running LLMs on a Mac will be
1. How much RAM the machine has, and
2. How much memory bandwidth the system provides

Obviously, in both cases, the more, the better. Other factors do also matter, e.g. the number of GPU cores packaged on the machine's SoC, and their generation (with GPU performance jumping about 35% going from M4 to M5, for example). Still, these are second-order considerations, with the sheer amount of RAM and the speed of access to it being most important (though this well might change once M5-Pro and M5-Max machines are released next year). 

Given how Apple's pricing structure works, there's really no good way to maximise the amount of RAM on a machine in any given line without also maximising the amount of memory bandwidth available for that RAM. In a pre-LLM world this would have been regarded as a huge demerit for Apple's offerings, but for the purposes of the current discussion, this pricing strategy can probably now be considered a **good** thing. I say this because it means people looking at getting 64, 96, 128 or even 256 GB of RAM won't be misled into thinking they'll be getting maximum value for their money by opting for build configurations without the bandwidth to get the most out of all that extra memory. 

To provide a concrete example, at present the maximum amount of RAM one can get with a 16-inch M4-Pro Macbook [is 48GB](https://www.apple.com/uk/shop/buy-mac/macbook-pro/16-inch-space-black-standard-display-apple-m4-pro-with-14-core-cpu-and-20-core-gpu-48gb-memory-512gb). Going up to 64 GB or higher requires selecting [an M4-Max](https://www.apple.com/uk/shop/buy-mac/macbook-pro/16-inch-space-black-standard-display-apple-m4-max-with-16-core-cpu-and-40-core-gpu-48gb-memory-1tb), but this also bumps up the available memory bandwidth from **273GB/s** to **546GB/s**. Even this higher number pales in comparison the 1,790GB/s available on an NVIDIA RTX 5090 card, but the NVIDIA card alone is currently listed at between £2,300 to £3,000 (assuming it's even available), and it comes with only 32 GB of that ultra-fast memory. In addition, the NVIDIA card can draw up to **575W** of power at peak usage, while an entire MacBook Pro peaks at **140W**. The NVIDIA RTX 5090 appears to be out of stock everywhere.

A similar story can be told about Apple's Mac Studio machines. An M4-Max version of the Studio with [128 GB of RAM](https://www.apple.com/uk/shop/buy-mac/mac-studio/apple-m4-max-with-14-core-cpu-32-core-gpu-16-core-neural-engine-36gb-memory-512gb) currently lists for £3,600 on Apple's website. Going up to 256 GB requires also moving up to an [M3-Ultra](https://www.apple.com/uk/shop/buy-mac/mac-studio/apple-m3-ultra-with-28-core-cpu-60-core-gpu-32-core-neural-engine-96gb-memory-1tb) version of the Mac Studio, but this also pushes up the system bandwidth from 410GB/s to 819GB/s. An M3 Ultra Mac Studio with 256 GB of RAM costs just £5,800, while a system with 512 GB can be had for just £9,700. It might seem outrageous that I use the term "just" in juxtaposition with such prices, but they really do offer good value for money, as I shall now explain.

A model running on a unified-memory-architecture with less memory-bandwidth will typically run faster than it would on a system which has an ultra-high bandwidth graphics card which lacks sufficient memory to keep the whole memory on the GPU. The larger the share of the model that must be offloaded onto much slower system memory, the bigger the hit will be. This issue tends to be exacerbated on the Intel machines available to consumers because these are almost always dual-channel systems which top out at 50-70 GB/s. Even the new entry-level M5 MacBook pro offers 153 GB/s more than 2x this amount of bandwidth, and the huge gap in bandwidth since the M-series machines were introduced is one of the reasons why they have offered so much better performance than Intel systems since Apple launched the M1.

With the above in mind, it is important to note that no single NVIDIA card available on the market offers 256 GB of RAM, not even high-end cards like the H100, H200 and B200 which are the subject of export controls to China. The maximum amount of RAM provided on any single NVIDIA card is 192 GB, and for that much NVIDIA charges a princely sum. For example, various online sources mention a single [NVIDIA B200 SXM 192 GB](https://www.techpowerup.com/gpu-specs/b200-sxm-192-gb.c4210) as selling for **$45-50,000**. Granted, that huge sum buys memory of the _extremely_ fast HBM3 variety (providing an astonishing **8.2 TB/s** of bandwidth), but it still sets a much lower limit on how big a model one can load on a single card than Apple's Mac Studios do. 

Using multiple NVIDIA cards **does** make it possible to load larger models (which is how the trillion-parameter models which make the news are deployed), but so does connecting multiple Apple machines via Thunderbolt 5. In addition, Apple's machines are freely available to any members of the public willing to pay for them ( and with generous consumer financing for the suitably qualified), and the Apple products are typically delivered to customers within a month of being ordered. In contrast, NVIDIA's cards are routinely mentioned in news articles as sold out many months in advance of production, even with their astronomical price tags.

The long and short of it is that Apple's hardware is a lot more reasonably priced than it might first appear, at least for the purpose of running inference on LLMs locally. The virtues of Apple's offerings only increase when one thinks about the power draw of even consumer-level NVIDIA cards, the space needed for the PC boxes to house said cards, the power which will be drawn by the resulting systems, and all the accompanying heat and noise they will generate (which will in turn necessitate thinking about how to keep the systems cool). At peak utilisation, even the highest end M3 Ultra Mac Studio **systems** draw less than **half** the power needed for the consumer-oriented RTX 5090, and they can sustain this performance with only their (relatively-quiet) internal fans to keep temperatures down.

## Making the Best of What You Have

Of course, all of the preceding discussion is moot if the hardware decision has already been made (e.g. because one already own the machine in question). If the primary usage for your machine is as a home device, then it's doubtful it will have more than 48 GB of RAM, even if, like me, you're the sort to simultaneously run an IDE, a database and several Docker images while having dozens of browser tabs open. My own personal machine is an M3-Pro MacBook Pro with 36 GB of unified memory, and despite the relatively low (for Apple) 150 GB/s of memory bandwidth on this machine, I am still able to productively work locally with 4-to-8 bit quantised versions of models like [GPT-OSS-20B](https://ollama.com/library/gpt-oss:20b), [Qwen3-8B](https://huggingface.co/Qwen/Qwen3-8B).

### My Personal Setup

My own reason for bothering with running anything locally is simply because I already own the needed hardware. Not having to buy any more equipment makes the marginal cost of playing around with quantised or intrinsically small models effectively zero,  lower than it could ever be to rent hardware on sites like [Runpod](https://www.runpod.io/) or [together.ai](https://www.together.ai/). As the primary driver for my getting into local inference has been for educational purposes - so I can get a feel for what is possible at various model sizes - I can live with waiting a little for my prompts to be processed, and don't mind putting up with rates of token production that would probably frustrate most of the "vibe coder" contingent. 

To be honest, the very idea of "vibe coding" holds no interest for me, being an old-fashioned type who desires a crystal-clear understanding of every line of code I'm responsible for. Nor would slow token output bother me very much even if I were a "vibe coder" in the making, as in my professional career I have never once encountered a situation in which the speed at which code is produced has ever been a constraint on how quickly I've met my development goals. As such, I am perfectly content letting a model take several minutes to do its thing in the background as I concentrate on something else, which is what my present setup allows me to do, however lacking it may be in comparison to, say, an M4-Max MacBook Pro with 128 GB of RAM and 576 GB/s of memory bandwidth to service it. 

Finally, nothing I want to do with LLMs requires me to handle sensitive private data, so if I really do require a model beyond my machine's capabilities, I can spare the \$0.50 per million input tokens (and \$1.75 per million output tokens) at which a cutting-edge model like [GLM-4.6](https://www.helicone.ai/llm-cost/provider/openrouter/model/z-ai%2Fglm-4.6) is currently being offered online. Even in the (extremely unlikely) scenario in which I had to run through a **billion** each of input and output tokens, I would still be spending a lot only \$2,250, less than **half** the £5,000 it would cost to buy a 128 GB M4-Max MacBook Pro to replace the hardware I currently own. 