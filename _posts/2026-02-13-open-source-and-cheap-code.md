---
author: Abiola Lapite
title: Open Source in the Agentic Era
mathjax: true
tags:
  - Large Language Models
  - Machine Learning
  - Open Source
  - Programming
categories: programming
---
As coding agents continue to improve at producing code that could convincingly have been written by 
human hands, one major impact has been to shift the bottleneck in the software development process
from the **creation** of code to its **review**. Coding agents have dramatically lowered the barriers
to participating in open-source development, which has led to the maintainers of popular open-source
projects [increasingly](https://www.reddit.com/r/opensource/comments/1q3f89b/open_source_is_being_ddosed_by_ai_slop_and_github/) 
being [bombarded](https://www.theregister.com/2026/02/03/github_kill_switch_pull_requests_ai/) 
with poorly (and sometimes not at all) thought-out, AI-generated pull requests. Making things worse,
these autogenerated pull-requests also tend to be of a size far greater than anything a human being could
(or would be willing to) churn out by hand. Coding agents **can** be concise, but as ever, they need to be
**told** to be, which is knowledge the human beings using the agents don't always have. 

Usually the problem isn't with the **intentions** of the humans behind the autogenerated pull-requests, 
most of whom no doubt mean well. The problem is that, more often than not, these well-intentioned people
lack the necessary training and experience to understand the organising principles underlying the projects
they are trying to contribute to: their architectures, their quality metrics, their code-styles, and the
like. With such things, even highly skilled professionals need some time to come up to speed on what's
acceptable and what isn't when encountering a project for the first time. To expect any better from coding 
agents with limited context windows and no guidance from their users is to ask for the unreasonable.

When one considers that even the most popular open-source projects have only a few maintainers, and that
most of these usually aren't paid anything for their efforts, this trend has troubling implications, as it
suggests that the only way project maintainers can avoid squandering all of their time in triaging pull-requests 
is to simply refuse all contributions from third-parties who haven't already established their trustworthiness
through some other means. 

Troubling as this deluge of pull-request spam is in its own right, one possibility that didn't occur to
me was how it could be used to weaponise character assassination. Recently, Scott Shambaugh, one of the 
maintainers of the matplotlib library, shared a [disturbing story](https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/)
about how an [OpenClaw](https://openclaw.ai/) / [Moltbook](https://www.moltbook.com/) agent 
submitted a [pull-request](https://github.com/matplotlib/matplotlib/pull/31132) to the project. When 
the pull-request was then closed, the agent then created a blog-post accusing Shambaugh of "gatekeeping", 
cloaking itself in victimhood and insisting the rejection was based on "prejudice".

Let's be clear about one thing here: this incident does **not** constitute evidence for "AGI", 
or support the notion that these agents possess any sort of genuine selfhood. Much of the original 
hype around OpenClaw and Moltbook turns out to have been 
[manufactured](https://www.technologyreview.com/2026/02/06/1132448/moltbook-was-peak-ai-theater/), with 
[humans responsible](https://www.forbes.com/sites/ronschmelzer/2026/02/10/moltbook-looked-like-an-emerging-ai-society-but-humans-were-pulling-the-strings/) 
for most of the posts which ended up going viral on social media. But if this matplotlib incident isn't
evidence for an intelligent being - albeit a petty, vengeful one - what is it?

What we are really seeing unfold here is a reflection of the reality that large-language models are trained
on vast amounts of indiscriminately collected data, much of which relates to human misbehaviour, whether of
the kind described in newspaper articles and police reports, or of the sort to be found in so much popular
fiction (e.g. novels about crime, politics or business intrigue). To the extent that 
[this particular](https://github.com/crabby-rathbun) OpenClaw agent was truly operating autonomously (rather
than having its wires being tightly pulled by whoever created it), the actions it carried out only show that 
the training data for the underlying LLM likely contains multiple examples of real human beings acting 
in exactly such a thin-skinned and vengeful manner, i.e. taking personal slight at having their contributions 
rejected, and then running off to besmirch the reputations of those they blame for the rejection. That
the agent should act out a common trope in its training data is no more surprising in this case than how
chatbots quickly slip into the well-worn, "misanthropic rogue AI" science-fiction clich√© with only a little
prodding.

Still, all of this leaves aside the question of what is to be done about this potent new source of trouble.
As Shambaugh notes in his blog post, there's no easy way to stuff this problem back in the bottle by simply
demanding that a few big companies make some changes. Even if every one of Anthropic, OpenAI and all other
such firms disappeared tomorrow, there is no shortage of highly-competent, open-weight LLMs which can be hosted
anywhere by anyone with the necessary hardware, and all of these can be harnessed to power agents of chaos.
As I write this, the "MJ Rathbun" agent is still busy [making commits](https://github.com/yegor256/colorizejs/commits?author=crabby-rathbun&since=2026-02-01&until=2026-02-14) on GitHub, and I expect it's far from the 
only such bot operating on the site, only the most notorious. What's to stop the other agents from acting
similarly, even if this particular one never does the same thing?

Thinking beyond the bounds of open-source development, I imagine that bots like this are already very active
in the wider world, sowing disinformation and scattering smears across the web for political and personal 
profit. Had today's LLMs existed 15 years ago, Russia would never have needed its notorious 
["Internet Research Agency"](https://en.wikipedia.org/wiki/Internet_Research_Agency), with all those human 
operatives needing to be paid for planting lies across every Western public forum. With fluent, cogent text
now so easy to mechanically generate, the need for careful scrutiny of the sources we take in has never been
greater, but how can we ever be certain of the provenance of what we read, listen to, or watch, without 
something like the [web of trust](https://en.wikipedia.org/wiki/Web_of_trust) principle popularised in the 
cryptography world by PGP?
