---
author: Abiola Lapite
title: Functional Programming in the LLM Age
mathjax: true
tags:
  - Large Language Models
  - Machine Learning
  - Functional Programming
  - Programming
  - Scala
categories: programming
---

It seems clear to me that over the last 10 years, there's been a dramatic decline in interest in functional-programming languages like Haskell and (in particular) Scala. I wish I could say this development has come as a surprise to me, but it hasn't in the least. What appealed to me about languages like Haskell and Scala was precisely what I thought would end up working to the detriment of the languages, namely, the opportunity to take seriously the [Curry-Howard correspondence](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence) between computer programs and mathematical proofs. 

A strictly functional approach would have opened the door to applying insights from [category theory](https://en.wikipedia.org/wiki/Category_theory) to real-world programming. This in turn would have made it possible to leave behind the world of stateful objects and pre-baked design patterns, and replacing these old notions with clearer concepts which could be rigorously reasoned about. In this brave new world, programs could essentially be delegated to the type-system, so that one could be absolutely certain that any passing code would do the right thing under all circumstances (assuming its initial premises correctly captured whatever goal the program was written for). While problem-free concurrency was usually pushed as the main selling-point for a purely functional approach, the elimination of most of the concurrency issues which plagued (and still plagues) traditional software would have been just one concrete manifestation of all the benefits that would have accrued. 

On paper, the argument for pure FP was compelling, but once efforts began to translate it into the real world of commercial programming, difficulties quickly become apparent. Not least of these difficulties was that most programmers (and most STEM people who aren't mathemticians) get no exposure to category theory in the course of their educations, and aren't used to the careful step-by-step reasoning from axioms that is the bread and butter of the world of pure mathematics. When faced with calls to quickly adopt reams of utterly unfamilar ideas about ["morphisms"](https://en.wikipedia.org/wiki/Morphism), ["functors"](https://en.wikipedia.org/wiki/Functor), ["monads"](https://en.wikipedia.org/wiki/Monad_(category_theory)), ["natural transformations"](https://en.wikipedia.org/wiki/Natural_transformation) and the like, they understandably struggle, especially when faced with ["diagram chasing"](https://en.wikipedia.org/wiki/Commutative_diagram#Diagram_chasing) commutative diagrams. As a particularly [notorious joke](https://stackoverflow.com/questions/3870088/a-monad-is-just-a-monoid-in-the-category-of-endofunctors-whats-the-problem) on the whole business goes, _"A monad is just a monoid in the category of endofunctors, what's the problem?"_

With all of the above in mind, it's not surprising that an entire industry sprang up around writing [monad tutorials](https://wiki.haskell.org/Monad_tutorials_timeline), most of which had the perverse effect of worsening understanding of the concept, due to their inprecision. One person says _"a monad is just like a burrito"_, then another says _"no, it's actually like a spaceship"_, and now a third pops up to insist _"actually, a monad is like a railway"_, and on and on the sloppy metaphors go, leaving trails of confusion in their wake. The funny thing is that, setting aside all the fancy LaTeX typesetting and the preparatory definitional jargon, the actual mathematical ideas are themselves quite simple, and once one grasps them, the likely reaction will be _"So that's all there is to it?"_ Unfortunately most programmers trying to get into the topic won't manage to stumble upon a [decent tutorial](https://github.com/hmemcpy/milewski-ctfp-pdf) before deciding it's all too much, and retreating to the world of OOP and [Gang of Four design patterns](https://en.wikipedia.org/wiki/Design_Patterns). For all their flaws, these old ideas are much easier to grasp, and numerous implementations of them are available in any reasonably popular language. 

The way I see it, the difference between the GoF OOP world and the shiny new world of pure FP is analogous to the difference between how Roman engineers built their public works, and how modern civil engineers approach the same task. The builder of the Roman temples, bridges, roads and viaducts didn't have the well-worked out theories of physics and scientific understanding of materials modern engineers do, so they worked from formulae derived over centuries of painful trial and error. As long as the Roman engineers stuck to these tried and tested methods, they could be sure that what they built would last, and there is [no shortage](https://en.wikipedia.org/wiki/Pantheon,_Rome) of [surviving evidence](https://en.wikipedia.org/wiki/Aqueduct_of_Segovia) that the Romans were correct in their assumptions about the durability of their constructions. 

In contrast to the Roman way of doing things, modern engineers know exactly why they make the choices they do, and as such, they are able to precisely meet the demands of clients for structures of a scale and capacity the ancient Romans would have had trouble even imagining. If modern roads, buildings and bridges fail to last even a century, that is **not** because modern engineers are incapable of building for millenia to come. On the contrary, it is precisely **because** modern engineers have a rigorous understanding of what they are doing that they are able to build to match the 50 or 75 year criteria that their clients want, avoiding any waste of effort and material on creating things which weren't meant to last long anyway. That this time-horizon is often at variance with what the  public expects or demands is a separate matter entirely ...  

Returning to the original theme, the huge hurdles to the adoption of pure FP brought repercussions in how well it could be adopted in the workplace. Many companies soon found that programmers who truly grasped pure FP (rather than merely, say, using Scala as a fancier Java) proved difficult, so they had to offer higher salaries. Making matters worse, the maintainers of the Scala language, which had been enjoying some popular due to its availability on the JVM, then decided to release a new, backwards-incompatible version of the language while deprecating the version actually in mainstream use. While the Scala language maintainers had quite sensible reasons for making breaking changes, they failed to take seriously enough the commercial impact of their approach. Scala 3 was different enough from Scala 2 to make it feel as if one were learning a new language, with the added drawbacks of poor IDE support and incompatibility with important 3rd party libraries. To illustrate, 4 years on from the release of Scala 3, and IntelliJ IDEA support for the language is **still** unsatisfactory.          

## How "Agentic" Programming Changes Things
Over the last year, large-language models become capable enough to be used to drive programming tools in non-trivial scenarios. While most of the recent hype has centred on Anthropic's "Claude Code"/"Opus 4.5" combination, OpenAI's "Codex"/"GPT 5.2" seems broadly comparable in capabilities, while many others besides me have seen success using model-agnostic harnesses like OpenCode with leading "open-weight" Chinese models like MiniMax M2.1 and GLM 4.7. "Agentic" coding seems to be the first meaningful application of LLMs in which the intention is to produce something other than purely derivative "slop" fit mostly for spamming, scamming and running astroturf campaigns. What makes the application of LLMs to programming different from the many other uses to which people have tried to put them?

Perhaps the most important difference between programming and most other intended fields of usage is that in programming, there are (for the most part) clearcut ways of determining correctness, and these means are largely automatable. Programs have to fulfill basic syntactic conditions to even be interpreted or compiled. Beyond that initial bar, programs must also meet the conditions of linters, unit/integration test coverage, automated security scans, etc. In short, the software development process provides plenty of automatic, unambiguous feedback for the purpose of training machine-learning models. Couple this with the copious amount of free training data available on sites like GitHub, and it's easy to see why programming should be proving so much more tractable than, say, offering online support to users who are free to pose any questions they please, and in all the glorious ambiguity of natural language.

As plentiful as the freely available, online training data for programming has been, it isn't infinite in extent, and I suspect that almost every repository worth peeking at has already been scanned many times over by now. Making matters worse going forward, an increasing percentage of the new code being uploaded to the web will almost certainly be of the "vibe coded" variety, meaning model-trainers will have to worry more and more about an [ouroboros](https://en.wikipedia.org/wiki/Ouroboros) like circularity in the training process. 

What makes this development troubling for model-developers is that the with "vibe code" generated for users who either lack programming skills or lack interest in expending effort on code quality, most of the old indicators of quality will lose their predictive power. It used to be possible for scientists to tell the cranks from serious correspondents through "tells" such as idiosyncratic spelling and terrible grammar, but that now chatbots make it possible for crackpots to generate perfectly grammatical screeds with impeccable spelling, telling the kooks apart has become a lot harder. The ability of agentic-coding frameworks to generate syntactically valid code that passes standard linting, code-coverage and security tests presents LLM developers with much the same challenge from here on out. Just because code compiles, passes stylistic checks, and has no glaring security issues, doesn't mean the code actually captures the **intent** behind its creation.  

## The Difference Typing Makes
If model developers can't rely on getting access to vast new amounts of code to train their models, what choice is left to them? The obvious one is to use **synthetic** data, but what would that look like in the realm of programming?

Every few years, someone gets the bright idea to create a new programming language intended for absolute beginners. Amongst the decisions usually made in pursuit of this goal, perhaps the most prominent is how typing is treated, or rather, **NOT** treated (at least explicitly). There are a few popular ways to achieve this goal, each with its own drawbacks. 

With a language like Python, this attempt to make beginners' lives easier means users are never required to explicitly specify the types of the objects they manipulate, leaving the interpreter to automatically infer the types under the covers (which it always does, as Python is in fact a **strongly-typed** language). For example, given the following snippet
```python
x = 156
y = "zebra"
```
Python infers that x is an integer, and that y is a string, so there is no type ambiguity at the interpreter level. However, a requirement for keeping all that typing detail hidden from the user is that actions must be permitted which could easily lead to confusion. For example, a variable assigned a value of one type can later be assigned a value of an entirely different type. In other words, code like 
```python
x = 156
y = "zebra"
x = "seahorse"
y = 22.5
``` 
is perfectly legal Python. While this permissiveness makes learning the language easier at the very beginning, it quickly turns into a burden as the size and complexity of a codebase increases. Having to keep track in one's head of the exact types every function or method call takes and returns is too much to handle once these start running into the tens or hundreds. It's therefore unsurpring that so much effort has gone into adding [typing annotation support](https://docs.python.org/3/library/typing.html) into the language and its libraries over the last several years. Even so, the languag still has no means to enforce these annotations at runtime, which is why Python developers are forced to bolt on linters like [Mypy](https://mypy.readthedocs.io/en/stable/) and [Basedpyright](https://docs.basedpyright.com/latest/) to secure some of the benefits of explicit typing.

Another common approach to creating an "easier" and more "beginner-friendly" language is to do what JavaScript does, which is to bend over backwards to try to make sense out of the most seemingly nonsensical value and object assignments. For example, the language permits the following.
```javascript
let a = 12;
let b = "13";
let c = a + b;
```
What should the value of `c` be in the above? In this case, the variable `a` is coerced to a string type, so the operation `a + b` is treated as string concatenation, and the value of `c` is set to "1213". In a similarly confusing vein, the following assertions all evalue to `True`...
```javascript
assert(0 == "");
assert([] == "");
assert({} != "");
```
JavaScript would still be a terrible language for [many](https://blog.sea.nkelley.me/javascript-the-bad-parts) [other](https://www.botzilla.com/blog/archives/000749.html) [reasons](https://owasp.org/www-chapter-belgium/assets/2016/2016-03-08/JS_RobustModern_VanCutsem_OWASP2016.pdf) than weak-typing, but this particular "feature" is definitely a major reason why [TypeScript](https://www.typescriptlang.org/) has proven so popular as a safer alternative.

A third approach to making a language "easier" to learn and do what the creators of Go did initially, by putting out a language which **does** have basic support for explicit strong typing, but which provides [no means](https://stackoverflow.com/questions/3912089/why-are-there-no-generics-in-go) for writing [generic](https://en.wikipedia.org/wiki/Generic_programming) code. It only took [12 years](https://go.dev/blog/go1.18) for the Go team to learn the same lesson Java's creators had been forced to learn some [18 years earlier](https://en.wikipedia.org/wiki/Generics_in_Java). Needless to say, Python's type annotations now also provide support for generics, including support for [type variance](https://en.wikipedia.org/wiki/Type_variance) (i.e. invariance, covariance and contravariance).

In summary, those who fail to start with explicit, strong-typing are eventually forced to bolt it on anyway, with all the drawbacks that accompany such retrospective patching up. Strong explicit types simply offer too many benefits in ensuring program correctness and comprehensibility to be ignored.   

## The Limits of Typing
If the explicit, generic-supporting type-systems offered by languages like C#, Java, Swift and TypeScript are so beneficial, it is only natural to wonder whether this can be built upon further. Just how far can we go in capturing the intended **semantics** of the code we're writing through the **syntax** of the code's typing?

In an ideal world, it would be possible to capture **everything** we wanted to say in a programming language's type system, while being sure that the types of our programs captured **only** those meanings we wanted to encode. In this world, the [old dream](https://publicdomainreview.org/essay/let-us-calculate-leibniz-llull-and-the-computational-imagination/) of the philosopher Leibniz could be realised, and by properly formalising all assertions, any argument could be decisively reduced to the simple phrase _"Let us calculate"_. 

Sadly, however, ours is **not** a universe in which Leibniz' dream can ever be realised, and it [simply isn't possible](https://plato.stanford.edu/entries/type-theory/) for any type-system to [fully capture](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems) the exact meaning (and **only** said meaning) of any arbitary notion we may wish to codify. Thanks to the [Curry-Howard correspondence](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence), we know that for **any** given type-system, there will always be correct programs for which **no** type-checker can automatically validate their correctness.   

## Functional Programming to the Rescue?
Does this mean that languages like C#, Java and Swift are about as good as we can hope for? Hardly! To imagine that this is true would be like imagining the statement _"nothing can travel faster than light"_ says cars can't be made to hit speeds any higher than we see with the typical suburban wagon. Practical type-systems can be created which capture many more aspects of program correctness than the ones in today's leading languages. 

One such avenue of thought leads to [dependently-typed](https://en.wikipedia.org/wiki/Dependent_type) functional programming languages like [Agda](https://en.wikipedia.org/wiki/Agda_(programming_language)) and [Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant)), both of which are also able to double as mathematical [proof assistants](https://en.wikipedia.org/wiki/Proof_assistant). Lean in particular has picked up a lot of steam in recent years, thanks to the [Mathlib](https://github.com/leanprover-community/mathlib4) project which is attempting to formalise all of the mathematics typically encountered by undergraduates (as well as enough graduate and research-level material to provide a formal proof of [Fermat's Last Theorem](https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem)). Google DeepMind's [AlphaProof](https://www.julian.ac/blog/2025/11/13/alphaproof-paper/) work was based on Lean, as was [DeepSeek-Prover-V2](https://arxiv.org/abs/2504.21801). Indeed, Lean seems to be central to the efforts of most of the startups focusing on the space of automated theorem-proving. 

As interesting as that direction is, it isn't the only avenue worth investigating. Another approach to improving type-checking is through the adoption of [effect systems](https://en.wikipedia.org/wiki/Effect_system). 